{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "increasing-wildlife",
   "metadata": {},
   "source": [
    "## Name\n",
    "### <u>Yi Huang</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-stretch",
   "metadata": {},
   "source": [
    "## Research Question/ Hypothesis\n",
    "### <u>Credit Card Fraud Detection with Unknown Feature Data</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-medicaid",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install category_encoders\n",
    "!pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "premier-laptop",
   "metadata": {},
   "outputs": [],
   "source": [
    "from   category_encoders          import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from   sklearn.compose            import *\n",
    "from   sklearn.ensemble           import RandomForestClassifier, GradientBoostingClassifier\n",
    "from   sklearn.experimental       import enable_iterative_imputer\n",
    "from   sklearn.impute             import *\n",
    "from   sklearn.metrics            import roc_auc_score # We have not covered it yet in class. The basics - AUC is from 0 to 1 and higher is better.\n",
    "from   imblearn.pipeline          import Pipeline\n",
    "from   sklearn.preprocessing      import *\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.decomposition   import PCA\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from   imblearn.pipeline          import make_pipeline\n",
    "from   sklearn.metrics            import balanced_accuracy_score\n",
    "import imblearn\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "hairy-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/yihuang1995/Fraud_Detection_with_Unknown_Features/main/creditcard_down.csv'\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "stylish-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "composite-concentrate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data with balanced class\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 42,test_size = 0.2,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-lounge",
   "metadata": {},
   "source": [
    "# Model1: PCA + Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-auckland",
   "metadata": {},
   "source": [
    "Due to 28 columns' definitions are unknown, for model1's feature engineering I applied PCA to demcompose the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "humanitarian-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need standardize the data before PCA, the data has no missing values\n",
    "pipe = make_pipeline(StandardScaler(),\n",
    "                     PCA(),\n",
    "                     imblearn.over_sampling.SMOTE(),\n",
    "                     RandomForestClassifier()\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "generic-research",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('pca', PCA(n_components=8)), ('smote', SMOTE()),\n",
       "                ('randomforestclassifier',\n",
       "                 RandomForestClassifier(bootstrap=False, criterion='entropy',\n",
       "                                        max_depth=5, min_samples_leaf=9,\n",
       "                                        n_estimators=250))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters = dict(randomforestclassifier__n_estimators     = [50,100,150,200,250,300], #control the number of trees\n",
    "                       randomforestclassifier__max_depth        = range(3, 10), # prevent over fitting\n",
    "                       randomforestclassifier__min_samples_leaf = range(1, 15), # prevent over fitting\n",
    "                       randomforestclassifier__bootstrap=[True,False], #whether bootstrap samples are used\n",
    "                       randomforestclassifier__criterion= [\"gini\", \"entropy\"],# criteria for splitting\n",
    "                       pca__n_components = range(2,9)) # control the output group of feature columns \n",
    "clf_rand_cv = RandomizedSearchCV(estimator=pipe, \n",
    "                              param_distributions=hyperparameters, \n",
    "                              n_iter=25, #25\n",
    "                              cv=5, \n",
    "                              n_jobs=-1,\n",
    "                              verbose=False)\n",
    "clf_rand_cv.fit(X_train, y_train)\n",
    "clf_rand_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "indie-variation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('pca', PCA(n_components=8)), ('smote', SMOTE()),\n",
       "                ('randomforestclassifier',\n",
       "                 RandomForestClassifier(bootstrap=False, criterion='entropy',\n",
       "                                        max_depth=5, min_samples_leaf=9,\n",
       "                                        n_estimators=250))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = clf_rand_cv.best_estimator_\n",
    "model1.fit(X_train,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "adaptive-torture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9316430125349633"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metric 1 balanced_accuracy_score\n",
    "y_pred1 = model1.predict(X_test)\n",
    "balanced_accuracy_score(y_test.values.ravel(), y_pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-collapse",
   "metadata": {},
   "source": [
    "# Model2: Feature Selection + GradientBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-mention",
   "metadata": {},
   "source": [
    "EDA shows that after performing log transformation, certain features' correlation with y get higher, I will perform column transformation on those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fancy-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_col = ['V8', 'V21', 'V22', 'V23', 'V27', 'V28', 'Amount']\n",
    "def log_transform(X):\n",
    "    return np.where(X>0, np.log(X), 0)\n",
    "preprocessing = ColumnTransformer(transformers=[('log', FunctionTransformer(log_transform, validate=False), log_col)], \n",
    "                                  remainder='passthrough')\n",
    "pipe2 = Pipeline([('log_transform',preprocessing),\n",
    "                 ('Sampling',imblearn.over_sampling.SMOTE()),\n",
    "                 ('GB',GradientBoostingClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "alien-suicide",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-63-15488216aa44>:3: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.where(X>0, np.log(X), 0)\n",
      "<ipython-input-63-15488216aa44>:3: RuntimeWarning: invalid value encountered in log\n",
      "  return np.where(X>0, np.log(X), 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('log_transform',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('log',\n",
       "                                                  FunctionTransformer(func=<function log_transform at 0x12181a8b0>),\n",
       "                                                  ['V8', 'V21', 'V22', 'V23',\n",
       "                                                   'V27', 'V28', 'Amount'])])),\n",
       "                ('Sampling', SMOTE()),\n",
       "                ('GB',\n",
       "                 GradientBoostingClassifier(max_depth=9, min_samples_leaf=4,\n",
       "                                            n_estimators=300, subsample=1))])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(divide = 'ignore',invalid = 'ignore')\n",
    "hyperparameters = dict(GB__n_estimators     = [50,100,150,200,250,300], #control the number of trees\n",
    "                       GB__max_depth        = range(3, 10), # prevent over fitting\n",
    "                       GB__min_samples_leaf = range(1, 10), # prevent over fitting\n",
    "                       GB__subsample = [0.6,0.8,1]) # sample number for fitting \n",
    "gb_rand_cv = RandomizedSearchCV(estimator=pipe2, \n",
    "                              param_distributions=hyperparameters, \n",
    "                              n_iter=25, #25\n",
    "                              cv=3, \n",
    "                              n_jobs=-1,\n",
    "                              verbose=False)\n",
    "gb_rand_cv.fit(X_train, y_train)\n",
    "gb_rand_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "respected-cemetery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('log_transform',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('log',\n",
       "                                                  FunctionTransformer(func=<function log_transform at 0x12181a8b0>),\n",
       "                                                  ['V8', 'V21', 'V22', 'V23',\n",
       "                                                   'V27', 'V28', 'Amount'])])),\n",
       "                ('Sampling', SMOTE()),\n",
       "                ('GB',\n",
       "                 GradientBoostingClassifier(max_depth=9, min_samples_leaf=4,\n",
       "                                            n_estimators=300, subsample=1))])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = gb_rand_cv.best_estimator_\n",
    "model2.fit(X_train,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "affected-beads",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9418470941676163"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metric 1 balanced_accuracy_score\n",
    "y_pred2 = model2.predict(X_test)\n",
    "balanced_accuracy_score(y_test.values.ravel(), y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-testimony",
   "metadata": {},
   "source": [
    "# Evaluation Metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bored-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "published-coral",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model1 Test Accuracy: 0.9316430125349633\n",
      "Model1 Test F1 score: 0.9090909090909092\n",
      "Model1 Test Precision: 0.9550561797752809\n",
      "Model1 Test Recall: 0.8673469387755102\n",
      "---------------------------\n",
      "Model2 Test Accuracy: 0.9418470941676163\n",
      "Model2 Test F1 score: 0.9206349206349207\n",
      "Model2 Test Precision: 0.9560439560439561\n",
      "Model2 Test Recall: 0.8877551020408163\n"
     ]
    }
   ],
   "source": [
    "balanced_accuracy_1 = balanced_accuracy_score(y_test.values.ravel(), y_pred1)\n",
    "f1_1 = f1_score(y_test.values.ravel(), y_pred1, average='binary')\n",
    "precision_1 = precision_score(y_test.values.ravel(), y_pred1, average='binary')\n",
    "recall_1 = recall_score(y_test.values.ravel(), y_pred1, average='binary')\n",
    "balanced_accuracy_2 = balanced_accuracy_score(y_test.values.ravel(), y_pred2)\n",
    "f1_2 = f1_score(y_test.values.ravel(), y_pred2, average='binary')\n",
    "precision_2 = precision_score(y_test.values.ravel(), y_pred2, average='binary')\n",
    "recall_2 = recall_score(y_test.values.ravel(), y_pred2, average='binary')\n",
    "\n",
    "print(\"Model1 Test Accuracy: {}\".format(balanced_accuracy_1))  # subset accuracy\n",
    "print(\"Model1 Test F1 score: {}\".format(f1_1))\n",
    "print(\"Model1 Test Precision: {}\".format(precision_1))\n",
    "print(\"Model1 Test Recall: {}\".format(recall_1))\n",
    "print('---------------------------')\n",
    "print(\"Model2 Test Accuracy: {}\".format(balanced_accuracy_2))  # subset accuracy\n",
    "print(\"Model2 Test F1 score: {}\".format(f1_2))\n",
    "print(\"Model2 Test Precision: {}\".format(precision_2))\n",
    "print(\"Model2 Test Recall: {}\".format(recall_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "centered-grace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[981,   4],\n",
       "       [ 11,  87]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test.values.ravel(), y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-observer",
   "metadata": {},
   "source": [
    "# Results\n",
    "### Best model: Feature Selection + GradientBoosting model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-novel",
   "metadata": {},
   "source": [
    "### Final Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cardiovascular-austria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('log_transform',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('log',\n",
       "                                                  FunctionTransformer(func=<function log_transform at 0x12181a8b0>),\n",
       "                                                  ['V8', 'V21', 'V22', 'V23',\n",
       "                                                   'V27', 'V28', 'Amount'])])),\n",
       "                ('Sampling', SMOTE()),\n",
       "                ('GB',\n",
       "                 GradientBoostingClassifier(max_depth=9, min_samples_leaf=4,\n",
       "                                            n_estimators=300, subsample=1))])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_rand_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-narrow",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* Model: Feature Selection + GradientBoosting model is slightly better than the PCA + RandomFrorest model. One reason might be that PCA reduces the information model can get from the data. Another reason is GradientBoosting algorithm might have better prediction power than the RandomForest.\n",
    "* Business: This is a fraud detection problem, we care the recall of the model, which means that how many fraud cases can be found from the model. 0.89 recall score means a pretty good model. If we are working as a vendor for our client, some features might be encrypted and unknown the true meaning of it, it is a good practice to know how to do feature engineering on those features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-carbon",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "* Deeper feature engineering: more transformation, combining features, add new features from original features\n",
    "* More complex model: Ensamble model, Stacked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-sculpture",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
